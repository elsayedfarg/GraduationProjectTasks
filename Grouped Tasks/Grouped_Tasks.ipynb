{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e5142345b00c49a3a2ec29911dcce3e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_16f92a36b6bb43aa8015347ce7ae67b1",
              "IPY_MODEL_7e068622978e4b3db99a07602890c340",
              "IPY_MODEL_01b8be79daeb499f94a3773fee3d62a0"
            ],
            "layout": "IPY_MODEL_393c670252db40db9c3d979cc0c10d3a"
          }
        },
        "16f92a36b6bb43aa8015347ce7ae67b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8939a46e43694b8fac75b40e0ed058d3",
            "placeholder": "​",
            "style": "IPY_MODEL_e7b503c3d8c2499c931dc565a7e002ac",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "7e068622978e4b3db99a07602890c340": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9db12642e8a54e27be8eb5c5f7b08a02",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_657f6dbe08844497b792fe007ad210e0",
            "value": 2
          }
        },
        "01b8be79daeb499f94a3773fee3d62a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb0fdb1021b34da4b00634401574978b",
            "placeholder": "​",
            "style": "IPY_MODEL_baf8ebb613414edbbf6159d923a23194",
            "value": " 2/2 [01:13&lt;00:00, 35.17s/it]"
          }
        },
        "393c670252db40db9c3d979cc0c10d3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8939a46e43694b8fac75b40e0ed058d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7b503c3d8c2499c931dc565a7e002ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9db12642e8a54e27be8eb5c5f7b08a02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "657f6dbe08844497b792fe007ad210e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "eb0fdb1021b34da4b00634401574978b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "baf8ebb613414edbbf6159d923a23194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DbJ-6mGaBzmr"
      },
      "outputs": [],
      "source": [
        "# ========================\n",
        "# 1. Auto Capture Script\n",
        "# ========================\n",
        "from IPython.display import Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import io\n",
        "from PIL import Image\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "\n",
        "def capture_multiple_images(num_frames=10, interval=2):\n",
        "    js_code = f'''\n",
        "        async function captureMultiplePhotos() {{\n",
        "          const div = document.createElement('div');\n",
        "          const video = document.createElement('video');\n",
        "          video.style.display = 'block';\n",
        "          const stream = await navigator.mediaDevices.getUserMedia({{video: true}});\n",
        "          document.body.appendChild(div);\n",
        "          div.appendChild(video);\n",
        "          video.srcObject = stream;\n",
        "          await video.play();\n",
        "\n",
        "          google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "          const canvas = document.createElement('canvas');\n",
        "          const context = canvas.getContext('2d');\n",
        "          const frames = [];\n",
        "\n",
        "          for (let i = 0; i < {num_frames}; i++) {{\n",
        "            canvas.width = video.videoWidth;\n",
        "            canvas.height = video.videoHeight;\n",
        "            context.drawImage(video, 0, 0);\n",
        "            frames.push(canvas.toDataURL('image/jpeg'));\n",
        "            await new Promise(resolve => setTimeout(resolve, {interval * 1000}));\n",
        "          }}\n",
        "\n",
        "          stream.getTracks().forEach(track => track.stop());\n",
        "          div.remove();\n",
        "          return frames;\n",
        "        }}\n",
        "        captureMultiplePhotos();\n",
        "    '''\n",
        "    data = eval_js(js_code)\n",
        "    images = []\n",
        "    for img_data in data:\n",
        "        binary = b64decode(img_data.split(',')[1])\n",
        "        img = Image.open(io.BytesIO(binary))\n",
        "        images.append(img)\n",
        "    return images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 2. Capture 10 Frames Automatically and Save\n",
        "# ================================\n",
        "frames = []\n",
        "interval = 2  # seconds between frames\n",
        "num_frames_to_capture = 10\n",
        "resize_shape = (224, 224)\n",
        "save_dir = \"captured_frames\"\n",
        "\n",
        "# Create folder if it doesn't exist\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "print(\"Starting automatic frame capture from webcam...\\n\")\n",
        "\n",
        "captured_images = capture_multiple_images(num_frames=num_frames_to_capture, interval=interval)\n",
        "\n",
        "for i, captured_image in enumerate(captured_images):\n",
        "    print(f\"Processing frame {i+1}/{num_frames_to_capture}\")\n",
        "\n",
        "    # Convert to OpenCV format\n",
        "    frame = cv2.cvtColor(np.array(captured_image), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Resize\n",
        "    frame_resized = cv2.resize(frame, resize_shape)\n",
        "\n",
        "    frames.append(frame_resized)\n",
        "\n",
        "    # Save frame to file\n",
        "    save_path = os.path.join(save_dir, f\"frame_{i}.jpg\")\n",
        "    cv2.imwrite(save_path, frame_resized)\n",
        "    print(f\"Saved: {save_path}\")\n",
        "\n",
        "print(\"\\nFinished capturing and saving frames!\")\n",
        "\n",
        "# Show array content summary\n",
        "print(f\"\\nTotal Frames Captured: {len(frames)}\")\n",
        "for i, f in enumerate(frames):\n",
        "    print(f\"Frame {i} shape: {f.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "sDRL77X_B7qH",
        "outputId": "a18f7e9d-ea4f-43c1-984f-a5dbcbc585ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting automatic frame capture from webcam...\n",
            "\n",
            "Processing frame 1/10\n",
            "Saved: captured_frames/frame_0.jpg\n",
            "Processing frame 2/10\n",
            "Saved: captured_frames/frame_1.jpg\n",
            "Processing frame 3/10\n",
            "Saved: captured_frames/frame_2.jpg\n",
            "Processing frame 4/10\n",
            "Saved: captured_frames/frame_3.jpg\n",
            "Processing frame 5/10\n",
            "Saved: captured_frames/frame_4.jpg\n",
            "Processing frame 6/10\n",
            "Saved: captured_frames/frame_5.jpg\n",
            "Processing frame 7/10\n",
            "Saved: captured_frames/frame_6.jpg\n",
            "Processing frame 8/10\n",
            "Saved: captured_frames/frame_7.jpg\n",
            "Processing frame 9/10\n",
            "Saved: captured_frames/frame_8.jpg\n",
            "Processing frame 10/10\n",
            "Saved: captured_frames/frame_9.jpg\n",
            "\n",
            "Finished capturing and saving frames!\n",
            "\n",
            "Total Frames Captured: 10\n",
            "Frame 0 shape: (224, 224, 3)\n",
            "Frame 1 shape: (224, 224, 3)\n",
            "Frame 2 shape: (224, 224, 3)\n",
            "Frame 3 shape: (224, 224, 3)\n",
            "Frame 4 shape: (224, 224, 3)\n",
            "Frame 5 shape: (224, 224, 3)\n",
            "Frame 6 shape: (224, 224, 3)\n",
            "Frame 7 shape: (224, 224, 3)\n",
            "Frame 8 shape: (224, 224, 3)\n",
            "Frame 9 shape: (224, 224, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 3. Load BLIP2 Model\n",
        "# ================================\n",
        "!pip install -q transformers accelerate\n",
        "\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-flan-t5-xl\",\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e5142345b00c49a3a2ec29911dcce3e1",
            "16f92a36b6bb43aa8015347ce7ae67b1",
            "7e068622978e4b3db99a07602890c340",
            "01b8be79daeb499f94a3773fee3d62a0",
            "393c670252db40db9c3d979cc0c10d3a",
            "8939a46e43694b8fac75b40e0ed058d3",
            "e7b503c3d8c2499c931dc565a7e002ac",
            "9db12642e8a54e27be8eb5c5f7b08a02",
            "657f6dbe08844497b792fe007ad210e0",
            "eb0fdb1021b34da4b00634401574978b",
            "baf8ebb613414edbbf6159d923a23194"
          ]
        },
        "id": "9XXMIBdkB8FC",
        "outputId": "232cd25d-df6d-4bd8-cc99-fb310a8e0d1f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5142345b00c49a3a2ec29911dcce3e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Blip2ForConditionalGeneration(\n",
              "  (vision_model): Blip2VisionModel(\n",
              "    (embeddings): Blip2VisionEmbeddings(\n",
              "      (patch_embedding): Conv2d(3, 1408, kernel_size=(14, 14), stride=(14, 14))\n",
              "    )\n",
              "    (encoder): Blip2Encoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-38): 39 x Blip2EncoderLayer(\n",
              "          (self_attn): Blip2Attention(\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "            (qkv): Linear(in_features=1408, out_features=4224, bias=True)\n",
              "            (projection): Linear(in_features=1408, out_features=1408, bias=True)\n",
              "          )\n",
              "          (layer_norm1): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
              "          (mlp): Blip2MLP(\n",
              "            (activation_fn): GELUActivation()\n",
              "            (fc1): Linear(in_features=1408, out_features=6144, bias=True)\n",
              "            (fc2): Linear(in_features=6144, out_features=1408, bias=True)\n",
              "          )\n",
              "          (layer_norm2): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (post_layernorm): LayerNorm((1408,), eps=1e-06, elementwise_affine=True)\n",
              "  )\n",
              "  (qformer): Blip2QFormerModel(\n",
              "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (encoder): Blip2QFormerEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): Blip2QFormerLayer(\n",
              "          (attention): Blip2QFormerAttention(\n",
              "            (attention): Blip2QFormerMultiHeadAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): Blip2QFormerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (crossattention): Blip2QFormerAttention(\n",
              "            (attention): Blip2QFormerMultiHeadAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): Blip2QFormerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate_query): Blip2QFormerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output_query): Blip2QFormerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): Blip2QFormerLayer(\n",
              "          (attention): Blip2QFormerAttention(\n",
              "            (attention): Blip2QFormerMultiHeadAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): Blip2QFormerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate_query): Blip2QFormerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output_query): Blip2QFormerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): Blip2QFormerLayer(\n",
              "          (attention): Blip2QFormerAttention(\n",
              "            (attention): Blip2QFormerMultiHeadAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): Blip2QFormerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (crossattention): Blip2QFormerAttention(\n",
              "            (attention): Blip2QFormerMultiHeadAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): Blip2QFormerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate_query): Blip2QFormerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output_query): Blip2QFormerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): Blip2QFormerLayer(\n",
              "          (attention): Blip2QFormerAttention(\n",
              "            (attention): Blip2QFormerMultiHeadAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): Blip2QFormerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate_query): Blip2QFormerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output_query): Blip2QFormerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): Blip2QFormerLayer(\n",
              "          (attention): Blip2QFormerAttention(\n",
              "            (attention): Blip2QFormerMultiHeadAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): Blip2QFormerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (crossattention): Blip2QFormerAttention(\n",
              "            (attention): Blip2QFormerMultiHeadAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): Blip2QFormerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate_query): Blip2QFormerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output_query): Blip2QFormerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): Blip2QFormerLayer(\n",
              "          (attention): Blip2QFormerAttention(\n",
              "            (attention): Blip2QFormerMultiHeadAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): Blip2QFormerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate_query): Blip2QFormerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output_query): Blip2QFormerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): Blip2QFormerLayer(\n",
              "          (attention): Blip2QFormerAttention(\n",
              "            (attention): Blip2QFormerMultiHeadAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): Blip2QFormerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (crossattention): Blip2QFormerAttention(\n",
              "            (attention): Blip2QFormerMultiHeadAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): Blip2QFormerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate_query): Blip2QFormerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output_query): Blip2QFormerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): Blip2QFormerLayer(\n",
              "          (attention): Blip2QFormerAttention(\n",
              "            (attention): Blip2QFormerMultiHeadAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): Blip2QFormerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate_query): Blip2QFormerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output_query): Blip2QFormerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): Blip2QFormerLayer(\n",
              "          (attention): Blip2QFormerAttention(\n",
              "            (attention): Blip2QFormerMultiHeadAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): Blip2QFormerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (crossattention): Blip2QFormerAttention(\n",
              "            (attention): Blip2QFormerMultiHeadAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): Blip2QFormerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate_query): Blip2QFormerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output_query): Blip2QFormerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): Blip2QFormerLayer(\n",
              "          (attention): Blip2QFormerAttention(\n",
              "            (attention): Blip2QFormerMultiHeadAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): Blip2QFormerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate_query): Blip2QFormerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output_query): Blip2QFormerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): Blip2QFormerLayer(\n",
              "          (attention): Blip2QFormerAttention(\n",
              "            (attention): Blip2QFormerMultiHeadAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): Blip2QFormerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (crossattention): Blip2QFormerAttention(\n",
              "            (attention): Blip2QFormerMultiHeadAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=1408, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=1408, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): Blip2QFormerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate_query): Blip2QFormerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output_query): Blip2QFormerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): Blip2QFormerLayer(\n",
              "          (attention): Blip2QFormerAttention(\n",
              "            (attention): Blip2QFormerMultiHeadAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): Blip2QFormerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate_query): Blip2QFormerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output_query): Blip2QFormerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (language_projection): Linear(in_features=768, out_features=2048, bias=True)\n",
              "  (language_model): T5ForConditionalGeneration(\n",
              "    (shared): Embedding(32128, 2048)\n",
              "    (encoder): T5Stack(\n",
              "      (embed_tokens): Embedding(32128, 2048)\n",
              "      (block): ModuleList(\n",
              "        (0): T5Block(\n",
              "          (layer): ModuleList(\n",
              "            (0): T5LayerSelfAttention(\n",
              "              (SelfAttention): T5Attention(\n",
              "                (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (relative_attention_bias): Embedding(32, 32)\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (1): T5LayerFF(\n",
              "              (DenseReluDense): T5DenseGatedActDense(\n",
              "                (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
              "                (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
              "                (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "                (act): GELUActivation()\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1-23): 23 x T5Block(\n",
              "          (layer): ModuleList(\n",
              "            (0): T5LayerSelfAttention(\n",
              "              (SelfAttention): T5Attention(\n",
              "                (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (1): T5LayerFF(\n",
              "              (DenseReluDense): T5DenseGatedActDense(\n",
              "                (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
              "                (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
              "                (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "                (act): GELUActivation()\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (final_layer_norm): T5LayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (decoder): T5Stack(\n",
              "      (embed_tokens): Embedding(32128, 2048)\n",
              "      (block): ModuleList(\n",
              "        (0): T5Block(\n",
              "          (layer): ModuleList(\n",
              "            (0): T5LayerSelfAttention(\n",
              "              (SelfAttention): T5Attention(\n",
              "                (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (relative_attention_bias): Embedding(32, 32)\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (1): T5LayerCrossAttention(\n",
              "              (EncDecAttention): T5Attention(\n",
              "                (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (2): T5LayerFF(\n",
              "              (DenseReluDense): T5DenseGatedActDense(\n",
              "                (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
              "                (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
              "                (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "                (act): GELUActivation()\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (1-23): 23 x T5Block(\n",
              "          (layer): ModuleList(\n",
              "            (0): T5LayerSelfAttention(\n",
              "              (SelfAttention): T5Attention(\n",
              "                (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (1): T5LayerCrossAttention(\n",
              "              (EncDecAttention): T5Attention(\n",
              "                (q): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (k): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (v): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "                (o): Linear(in_features=2048, out_features=2048, bias=False)\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (2): T5LayerFF(\n",
              "              (DenseReluDense): T5DenseGatedActDense(\n",
              "                (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n",
              "                (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n",
              "                (wo): Linear(in_features=5120, out_features=2048, bias=False)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "                (act): GELUActivation()\n",
              "              )\n",
              "              (layer_norm): T5LayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (final_layer_norm): T5LayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (lm_head): Linear(in_features=2048, out_features=32128, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# 4. Predicting on Captured Frames\n",
        "# ================================\n",
        "print(\"\\nMaking predictions on captured frames...\\n\")\n",
        "\n",
        "for idx, frame in enumerate(frames):\n",
        "    # Convert OpenCV BGR to PIL RGB\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    pil_image = Image.fromarray(frame_rgb)\n",
        "\n",
        "    inputs = processor(pil_image, return_tensors=\"pt\").to(device, torch.float16)\n",
        "\n",
        "    # Generate caption\n",
        "    generated_ids = model.generate(**inputs)\n",
        "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "\n",
        "    print(f\"Prediction for Frame {idx}: {generated_text}\")\n",
        "\n",
        "print(\"\\n All done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Wa0c5l0B-zq",
        "outputId": "a1bebbe0-4b71-4e9c-ee11-70c1dbf8a8fd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Making predictions on captured frames...\n",
            "\n",
            "Prediction for Frame 0: a man with a black hair and a black shirt is sitting on a bed\n",
            "Prediction for Frame 1: a man with a black shirt and a black shirt\n",
            "Prediction for Frame 2: a man with his hands in his pocket\n",
            "Prediction for Frame 3: a man with his hands raised in the air\n",
            "Prediction for Frame 4: a man with a black shirt and a black shirt is sitting on a bed\n",
            "Prediction for Frame 5: a man with a tee shirt and a black shirt\n",
            "Prediction for Frame 6: a man is pointing his finger at a camera in a room\n",
            "Prediction for Frame 7: a man with a clenched fist and a slapped\n",
            "Prediction for Frame 8: a man with a black shirt and a black shirt is sitting on a bed\n",
            "Prediction for Frame 9: a man with a black shirt and a black shirt is sitting on a bed\n",
            "\n",
            " All done!\n"
          ]
        }
      ]
    }
  ]
}